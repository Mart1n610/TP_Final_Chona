{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including TensorFlow, Keras, and others needed for data manipulation and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W2sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m \u001b[39m# Importing necessary libraries\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W2sdW50aXRsZWQ%3D?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W2sdW50aXRsZWQ%3D?line=2'>3</a>\u001b[0m \u001b[39m# TensorFlow and Keras for building the CNN\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W2sdW50aXRsZWQ%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W2sdW50aXRsZWQ%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W2sdW50aXRsZWQ%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "# TensorFlow and Keras for building the CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# Libraries for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Libraries for handling images\n",
    "from PIL import Image\n",
    "\n",
    "# Libraries for splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Libraries for data preprocessing\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Setting the seed for reproducibility\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Dataset\n",
    "Load the dataset from the specified path and preprocess it. This includes resizing images, normalizing pixel values, and one-hot encoding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "dataset_path = 'C:\\Users\\47650557\\Documents\\GitHub\\TP_Final_Chona\\data'\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Define the image size to which we want to resize\n",
    "img_size = 48\n",
    "\n",
    "# Initialize lists to store the images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over the dataset and preprocess the images\n",
    "for index, row in data.iterrows():\n",
    "    # Convert the pixel values to a numpy array\n",
    "    pixels = np.array(row['pixels'].split(), dtype='uint8')\n",
    "    \n",
    "    # Reshape the pixel array into a 48x48 image\n",
    "    image = pixels.reshape((img_size, img_size))\n",
    "    \n",
    "    # Normalize the pixel values\n",
    "    image = image / 255.0\n",
    "    \n",
    "    # Append the image and label to the respective lists\n",
    "    images.append(image)\n",
    "    labels.append(row['emotion'])\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# One-hot encode the labels\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Reshape the data to fit the model\n",
    "X_train = X_train.reshape(-1, img_size, img_size, 1)\n",
    "X_test = X_test.reshape(-1, img_size, img_size, 1)\n",
    "\n",
    "# Create an image data generator for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "# Fit the data generator to the training data\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the CNN Model\n",
    "Define the architecture of the CNN. This includes adding convolutional layers, max pooling layers, dropout layers, and fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first convolutional layer\n",
    "model.add(Conv2D(64, (3, 3), padding='same', input_shape=(img_size, img_size, 1)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add the first max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Add the second convolutional layer\n",
    "model.add(Conv2D(128, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add the second max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Add the third convolutional layer\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add the third max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Add the fourth convolutional layer\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add the fourth max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flatten the tensor output from the last convolutional layer before passing it to the fully connected layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add the first fully connected layer\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Add the second fully connected layer\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Add the output layer with 7 neurons (for the 7 classes of emotions)\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the Model\n",
    "Compile the model with appropriate loss function, optimizer, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Train the model on the training data, using the validation data to validate the results after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs and batch size\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Evaluate the model's performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "# Print the test accuracy\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "# Generate predictions for the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert the predictions from categorical back to original form\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Import the necessary library for plotting the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model with New Images\n",
    "Use the trained model to predict emotions on new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries for handling images\n",
    "from PIL import Image\n",
    "import %pip install opencv-python\n",
    "cv2\n",
    "\n",
    "# Define the emotion labels\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Define the path to the new images\n",
    "new_images_path = '/path/to/new/images'\n",
    "\n",
    "# Initialize a list to store the new images\n",
    "new_images = []\n",
    "\n",
    "# Load the new images\n",
    "for image_path in new_images_path:\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Resize the image to 48x48\n",
    "    image = image.resize((48, 48))\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Normalize the pixel values\n",
    "    image = image / 255.0\n",
    "    \n",
    "    # Append the image to the list\n",
    "    new_images.append(image)\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "new_images = np.array(new_images)\n",
    "\n",
    "# Reshape the data to fit the model\n",
    "new_images = new_images.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# Use the model to predict the emotions of the new images\n",
    "new_predictions = model.predict(new_images)\n",
    "\n",
    "# Convert the predictions from categorical back to original form\n",
    "new_predictions_classes = np.argmax(new_predictions, axis=1)\n",
    "\n",
    "# Print the predicted emotions\n",
    "for i, prediction in enumerate(new_predictions_classes):\n",
    "    print(f'Image {i+1}: {emotion_labels[prediction]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including TensorFlow, Keras, NumPy, pandas, matplotlib, seaborn, PIL, and sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "# TensorFlow and Keras for building the CNN model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# NumPy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib and seaborn for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PIL for image manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# Sklearn for data preprocessing and model evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Dataset\n",
    "Load the dataset from the GitHub repository and preprocess it using Keras utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing additional necessary libraries\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "# URL of the dataset on GitHub\n",
    "dataset_url = \"https://github.com/path_to_dataset/dataset.zip\"\n",
    "\n",
    "# Download the dataset\n",
    "r = requests.get(dataset_url)\n",
    "\n",
    "with open(\"dataset.zip\", \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "# Extract the dataset\n",
    "with zipfile.ZipFile(\"dataset.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"dataset\")\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = \"dataset\"\n",
    "\n",
    "# Define the image size\n",
    "image_size = (48, 48)\n",
    "\n",
    "# Initialize the ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Rescale the pixel values from [0, 255] to [0, 1]\n",
    "    validation_split=0.2  # Reserve 20% of the images for validation\n",
    ")\n",
    "\n",
    "# Load the training data\n",
    "train_data = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=image_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "# Load the validation data\n",
    "validation_data = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=image_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Dataset into Training and Testing Sets\n",
    "Use sklearn's train_test_split function to split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no need to split the dataset into training and testing sets in this section\n",
    "# The ImageDataGenerator already does this with the 'validation_split' argument\n",
    "# The 'train_data' variable contains the training set\n",
    "# The 'validation_data' variable contains the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the CNN Model\n",
    "Define the structure of the CNN using Keras, including the convolutional, max pooling, and dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN Model\n",
    "model = keras.models.Sequential([\n",
    "    # First convolutional layer\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    # Second convolutional layer\n",
    "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # Third convolutional layer\n",
    "    keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # Flatten the results to feed into a DNN\n",
    "    keras.layers.Flatten(),\n",
    "    \n",
    "    # 512 neuron hidden layer\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    \n",
    "    # Output layer with 7 neurons for 7 classes\n",
    "    keras.layers.Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the Model\n",
    "Compile the CNN model, specifying the optimizer, loss function, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',  # Adam optimizer\n",
    "    loss='categorical_crossentropy',  # Categorical crossentropy as it's a multi-class classification\n",
    "    metrics=['accuracy']  # Accuracy as a metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Train the CNN model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs and batch size\n",
    "epochs = 25\n",
    "batch_size = 64\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch=train_data.samples // batch_size,\n",
    "    validation_data=validation_data,\n",
    "    validation_steps=validation_data.samples // batch_size,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('emotion_detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Evaluate the performance of the trained CNN model on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = keras.models.load_model('emotion_detection_model.h5')\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_loss, test_accuracy = model.evaluate(validation_data)\n",
    "\n",
    "# Print the test loss and test accuracy\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict the classes of the testing data\n",
    "y_pred = model.predict(validation_data)\n",
    "\n",
    "# Convert the predictions to class labels\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert the testing data to class labels\n",
    "y_true = np.argmax(validation_data.labels, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_true, y_pred_classes))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Model's Performance\n",
    "Use matplotlib and seaborn to visualize the model's performance metrics and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "# END: be15d9bcejpp\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
